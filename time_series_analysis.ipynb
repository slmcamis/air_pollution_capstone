{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#import seaborn; seaborn.set()\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Modeling\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "#import statsmodels.tsa.api as smt\n",
    "import scipy.stats as scs\n",
    "\n",
    "# Progress bars\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Remove warnings\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_annual_data(measure):\n",
    "    \"\"\"\n",
    "    Combines files for each year into one big file for given measure\n",
    "    \n",
    "    Input:\n",
    "    measure (string) - ['co', 'so2', 'no2', 'o3', 'pm10', 'pm25', \n",
    "                        'aqi', 'pressure', 'temperature', 'wind']\n",
    "    \n",
    "    Output:\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    measure_codes = {'co': '42101', 'so2': '42401', 'no2': '42602', 'o3': '44201', \n",
    "                     'pm10': '81102', 'pm25': '88101', 'aqi': 'aqi_by_county',\n",
    "                     'pressure': 'PRESS', 'temperature': 'TEMP', 'wind': 'WIND'}\n",
    "    prefix = 'all_data\\daily_'\n",
    "    code = measure_codes.get(measure)\n",
    "    suffix = '.csv'\n",
    "    filenames = []\n",
    "    for i in range(20):\n",
    "        year = 2000+i\n",
    "        file = prefix + code + '_' + str(year) + suffix\n",
    "        print(f'Adding file {file}')\n",
    "        filenames.append(file)\n",
    "    print('Combining files...')\n",
    "    df = pd.concat([pd.read_csv(f) for f in filenames], ignore_index = True)\n",
    "    print(f'Done! \\nShape of dataframe for {measure} for 2000-2019: {df.shape}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_column_names(df):\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merge_column(df):\n",
    "    df['merge_column'] = [f'{x}-{y}-{z}' for x, y, z in zip(df['state_name'], df['county_name'], df['date_local'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to a usable format for time series analysis\n",
    "def make_time_series(df):\n",
    "    '''\n",
    "    Input:\n",
    "    Dataframe\n",
    "    \n",
    "    Return:\n",
    "    Time series format with \n",
    "    index set as Date and \n",
    "    columns for arithmetic_mean of pollutant\n",
    "    '''\n",
    "    ts = pd.DataFrame()\n",
    "    #Set column Date to dates in df, convert to datetime\n",
    "    ts['Date'] = pd.to_datetime(df.date_local, format='%Y-%m-%d')\n",
    "    ts['arithmetic_mean'] = df.arithmetic_mean\n",
    "    ts.set_index('Date',inplace=True)\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries of pollutant\n",
    "def plot_time_series(ts, pollutant=pollutant, region_name=None, line=True, figsize=(12,8)):\n",
    "    '''\n",
    "    Plot line graph and boxplot of time series region\n",
    "    \n",
    "    Input:\n",
    "    ts: time series format\n",
    "    region_name: string, name of target region\n",
    "    line: boolean, plot line graph\n",
    "    figsize: default (12,8)\n",
    "    '''\n",
    "    if line:\n",
    "        # Generate line graph for each zipcode\n",
    "        ts.plot(figsize=figsize)\n",
    "        if region_name:\n",
    "            plt.title(f\"Average Daily {pollutant} Level in {region_name}\")\n",
    "        else:\n",
    "            plt.title(f\"Average Daily {pollutant} Level\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(ts, pollutant=pollutant, region_name=None, line=True, figsize=(12,8)):\n",
    "    # Generate a box and whiskers plot for each zipcode\n",
    "    ts.boxplot(figsize = figsize)\n",
    "    if region_name:\n",
    "        plt.title(f\"Average Daily {pollutant} Level in {region_name}\")\n",
    "    else:\n",
    "        plt.title(f\"Average Daily {pollutant} Level\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_time_series(ts, period=1, figsize=(12,4)):\n",
    "    decomposition = seasonal_decompose(ts, period=period)\n",
    "\n",
    "    trend = decomposition.trend\n",
    "    seasonal = decomposition.seasonal\n",
    "    residual = decomposition.resid\n",
    "\n",
    "    trend.plot(figsize=figsize)\n",
    "    plt.title(\"Trend\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.show()\n",
    "\n",
    "    seasonal.plot(figsize=figsize)\n",
    "    plt.title(\"Seasonality\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.show()\n",
    "\n",
    "    residual.plot(figsize=figsize)\n",
    "    plt.title(\"Residual\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pacf_plot(ts,lags=100):\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    sm.graphics.tsa.plot_pacf(ts, ax=ax, lags=lags)\n",
    "    return\n",
    "\n",
    "\n",
    "def acf_plot(ts,lags=100):\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    sm.graphics.tsa.plot_acf(ts, ax=ax, lags=lags)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ts, len_test):\n",
    "    train, test = ts[:-len_test], ts[-len_test:]\n",
    "    print(\"Train Test Split Complete \\nLength of Train: {} \\tLength of Test: {}\".format(len(train), len(test)))\n",
    "    return train, test\n",
    "\n",
    "# create a list of parameters to try\n",
    "def param_combinations(param_range=3, differencing_range=2, seasonal=[0,12], trend=[None,'t','ct']):\n",
    "    '''\n",
    "    Creates combinations of parameters for SARIMAX modeling\n",
    "    \n",
    "    Input: \n",
    "        param_range: int, range for p, q, P, Q (default 3=[0,1,2])\n",
    "        differencing_range: int, range for d and D (default 2=[0,1])\n",
    "        seasonal: list, default [0,12]\n",
    "        trend: list, default [None,'t','ct']\n",
    "            None - SARIMAX default\n",
    "            't' - linear trend\n",
    "            'ct' - linear trend with constant\n",
    "            *Note: to use only None, enter [None]\n",
    "        \n",
    "    Return:\n",
    "        list in this format: [(p,d,q), (P,D,Q,s), t]\n",
    "    '''\n",
    "    p = q = P = Q = range(param_range) #default 3 [0,1,2]\n",
    "    d = D = range(differencing_range) #default 2 [0,1]\n",
    "    s = seasonal #default [0,12]\n",
    "    t = trend #default [None,'t','ct']\n",
    "    params = []\n",
    "    # create config instances\n",
    "    for p_ in tqdm(p, desc='Making parameter combinations', leave=False):\n",
    "        for d_ in d:\n",
    "            for q_ in q:\n",
    "                for t_ in t:\n",
    "                    for P_ in P:\n",
    "                        for D_ in D:\n",
    "                            for Q_ in Q:\n",
    "                                for s_ in s:\n",
    "                                    combo = [(p_,d_,q_), (P_,D_,Q_,s_), t_]\n",
    "                                    params.append(combo)\n",
    "    return params\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(true_values, predictions):\n",
    "    return np.sqrt(mean_squared_error(true_values, predictions))\n",
    "\n",
    "def sarimax(ts, order, sorder, trend=None):\n",
    "    '''\n",
    "    Fits a Statsmodels SARIMAX Time Series Model.\n",
    "    \n",
    "    Inputs:\n",
    "        ts: time series format\n",
    "        order: list containing the p,d,and q values for the SARIMAX function.\n",
    "        sorder: list containing the seasonal p,d and q values along with seasonal \n",
    "            shift amount for the SARIMAX function\n",
    "        trend: string, options=[None, 'n', 'c', 'ct', 't']\n",
    "    \n",
    "    Return:\n",
    "        fitted model\n",
    "    '''\n",
    "    #Run and fit SARIMAX model with given parameters\n",
    "    model = sm.tsa.statespace.SARIMAX(ts, order=order, seasonal_order=sorder, trend=trend, \n",
    "                                      enforce_stationarity=False, \n",
    "                                      enforce_invertibility=False)\n",
    "    fit_model = model.fit(disp=False)\n",
    "    return fit_model\n",
    "\n",
    "def cross_validation_rmse(ts, order, sorder, trend=None, n_splits=4):\n",
    "    '''\n",
    "    Uses time series cross validation (TimeSeriesSplit) of n_splits.\n",
    "    Calculates RMSE for each split\n",
    "    *Remember to reserve a portion of the data for final model evaluation\n",
    "    \n",
    "    Input:\n",
    "        ts - full time series\n",
    "        order - ints, (#,#,#)\n",
    "        sorder - ints, (#,#,#,#)\n",
    "        trend - string\n",
    "        n_splits - number of cross validation splits (default 4)\n",
    "        \n",
    "    Returns:\n",
    "        Average root mean squared error of cross validations \n",
    "    '''\n",
    "    #Initialize TimeSeriesSplit with n_splits (default 4)\n",
    "    tscv = TimeSeriesSplit(n_splits = n_splits)\n",
    "    #Create list for RMSE of each cross validation split\n",
    "    rmse = []\n",
    "    try:\n",
    "        #Use tscv to implement Forward Chaining Nested Cross Validation\n",
    "        for train_index, test_index in tscv.split(ts):\n",
    "            #Make train, test split for section\n",
    "            cv_train, cv_test = ts.iloc[train_index], ts.iloc[test_index]\n",
    "            #Run and fit model for section\n",
    "            model = sarimax(cv_train, order, sorder, trend)\n",
    "            #Get predictions from model for cv_test date range\n",
    "            predictions = model.predict(cv_test.index.values[0], cv_test.index.values[-1])\n",
    "            #Store true values for cv_test date range\n",
    "            true_values = cv_test.values\n",
    "            #Calculate RMSE and append to list\n",
    "            rmse.append(measure_rmse(true_values, predictions))\n",
    "        #Calculate average of RMSEs from cross validations\n",
    "        cv_rmse = round(sum(rmse)/n_splits,3)\n",
    "    except:\n",
    "        cv_rmse = np.nan\n",
    "    \n",
    "    return cv_rmse\n",
    "\n",
    "def results_dict(ts, order, sorder, trend):\n",
    "    cv_rmse = cross_validation_rmse(ts, order, sorder, trend)\n",
    "    model = sarimax(ts, order, sorder, trend)\n",
    "    aic = model.aic\n",
    "    bic = model.bic\n",
    "    dictionary = {'model':model,'order':order,'sorder':sorder,'trend':trend,\n",
    "                'AIC':aic,'BIC':bic, 'CVRMSE':cv_rmse}\n",
    "    return dictionary\n",
    "\n",
    "def results_dict(ts_all, order, sorder, trend):\n",
    "    train_test_split_test_split()\n",
    "    cv_rmse = cross_validation_rmse(ts, order, sorder, trend)\n",
    "    model = sarimax(ts, order, sorder, trend)\n",
    "    aic = model.aic\n",
    "    bic = model.bic\n",
    "    dictionary = {'model':model,'order':order,'sorder':sorder,'trend':trend,\n",
    "                'AIC':aic,'BIC':bic, 'CVRMSE':cv_rmse}\n",
    "    return dictionary\n",
    "\n",
    "def run_models_by_params(ts, param_combos):\n",
    "    '''\n",
    "    Function to run SARIMAX model with cross validation for all parameter combinations\n",
    "    for single time series\n",
    "    Input:\n",
    "        ts: single time series (training data)\n",
    "        param_combos: list of parameter combinations\n",
    "            format: order, sorder, trend\n",
    "            \n",
    "    '''\n",
    "    #Initialize list for results\n",
    "    results = []\n",
    "    #Iterate through parameters with progress baar\n",
    "    for param in tqdm(param_combos, desc='Running models with cross validation', leave=False):\n",
    "        #Separate parameters\n",
    "        order, sorder, trend = param\n",
    "        #Run model with cross validation\n",
    "        result = results_dict(ts, order, sorder, trend)\n",
    "        #Add dictionary of result to list\n",
    "        results.append(result)\n",
    "    #Convert list of dictionaries to dataframe\n",
    "    df = pd.DataFrame(results, columns=['model', 'order', 'sorder', 'trend', 'AIC', 'BIC', 'CVRMSE'])\n",
    "    return df\n",
    "\n",
    "def run_all_models(ts_all, param_combos):\n",
    "    '''\n",
    "    Function to iterate through zipcodes and run SARIMAX models with cross validation \n",
    "    for all combinations of parameters\n",
    "    \n",
    "    Input:\n",
    "        ts - time series of region (training data)\n",
    "        param_combos - list of parameter combinations\n",
    "    '''\n",
    "    #Initialize list for rows\n",
    "    df_list = []\n",
    "    #Iterate through the columns in the time seres\n",
    "    for zipcode in tqdm(ts_all.columns, desc='Modeling zipcodes', leave=False):\n",
    "        #Sanity check\n",
    "        print(f'Running models for zipcode {zipcode}')\n",
    "        #Isolate the data for the zipcode\n",
    "        ts = ts_all[zipcode]\n",
    "        #Iterate through all param_combos using time series cross validation\n",
    "        #Stores row as dataframe with results of model\n",
    "        zip_df = run_models_by_params(ts, param_combos)\n",
    "        #Add column for zipcode to dataframe\n",
    "        zip_df.insert(0, 'zipcode', zipcode)\n",
    "        #Add row to df_list\n",
    "        df_list.append(zip_df)\n",
    "    #Combine zip_df into dataframe\n",
    "    df = pd.concat(df_list)\n",
    "    return df\n",
    "\n",
    "def sort_best_models(results_df, criterion, drop_duplicates=True):\n",
    "    '''\n",
    "    Input:\n",
    "        df - dataframe of all model results\n",
    "        criterion - string, column name to sort by\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe of best model results for each zipcode\n",
    "    '''\n",
    "    df = results_df.copy()\n",
    "    #Drop nan values in CVRMSE columns\n",
    "    df.dropna(subset=['CVRMSE'], inplace=True)\n",
    "    #Sort values by given criterion\n",
    "    df.sort_values(criterion, ascending=True, inplace=True)\n",
    "    if drop_duplicates:\n",
    "        #Get top row for each zipcode\n",
    "        df.drop_duplicates(['zipcode'], inplace=True)\n",
    "    #Fill null values with None (affects trend)\n",
    "    df.fillna('None', inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_params(results_df, zipcode):\n",
    "    '''\n",
    "    Input:\n",
    "        best_results - Dataframe from running models\n",
    "        zipcode - int, 5 digits\n",
    "        \n",
    "    Returns:\n",
    "        order, seasonal_order, trend\n",
    "    '''\n",
    "    row = results_df.loc[results_df['zipcode']==zipcode]\n",
    "    order = (int(row['order'].values[0][1]),\n",
    "            int(row['order'].values[0][4]),\n",
    "            int(row['order'].values[0][7]))\n",
    "    sorder = (int(row['sorder'].values[0][1]),\n",
    "            int(row['sorder'].values[0][4]),\n",
    "            int(row['sorder'].values[0][7]),\n",
    "            int(row['sorder'].values[0][10:12]))\n",
    "    trend = str(row['trend'].values[0])    \n",
    "    #print(f'zipcode: {zipcode}, order: {order}, sorder: {sorder}, trend: {trend}')\n",
    "    \n",
    "    return order,sorder,trend\n",
    "\n",
    "def rmse_final(train, test, order, sorder, trend=None):\n",
    "    model = sm.tsa.statespace.SARIMAX(train, order=order, seasonal_order=sorder, trend=trend, \n",
    "                                      enforce_stationarity=False, \n",
    "                                      enforce_invertibility=False)\n",
    "    fit_model = model.fit(disp=False)\n",
    "    #Get predictions from model for test date range\n",
    "    predictions = fit_model.predict(test.index.values[0], test.index.values[-1])\n",
    "    #Store true values for test date range\n",
    "    true_values = test.values\n",
    "    #Calculate RMSE\n",
    "    rmse = measure_rmse(true_values, predictions)\n",
    "    return rmse\n",
    "\n",
    "def add_rmse_to_final_models(best_results, train, test):\n",
    "    #Add final RMSE for each model\n",
    "    rmse_list = []\n",
    "    best_results.dropna(subset=['CVRMSE'], inplace=True)\n",
    "    for zipcode in best_results.zipcode:\n",
    "        o,s,t = extract_params(best_results, zipcode)\n",
    "        rmse = rmse_final(train[zipcode], test[zipcode], o, s, None)\n",
    "        rmse_list.append(rmse)\n",
    "    best_results['RMSE'] = np.round(rmse_list,1)\n",
    "\n",
    "def plot_predictions(best_results_cvrmse, ts_all, start_test='2017-04', \n",
    "                     start_pred='2017-06-01', end_pred=None, dynamic=False):\n",
    "    fig, axes = plt.subplots(nrows=9, ncols=2, figsize=(15,65))\n",
    "    axes_list = [item for sublist in axes for item in sublist] \n",
    "    train, test = train_test_split(ts_all, 12)\n",
    "    for zipcode in best_results_cvrmse.zipcode:\n",
    "        ax = axes_list.pop(0)\n",
    "        order,sorder,trend = extract_params(best_results_cvrmse, zipcode)\n",
    "        output = sarimax(ts_all[zipcode], order, sorder)\n",
    "        pred = output.get_prediction(start=start_pred, end=end_pred, dynamic=dynamic)\n",
    "        pred_ci = pred.conf_int()\n",
    "        test[zipcode][start_test:].plot(label='Observed', ax=ax)\n",
    "        pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "        ax.fill_between(pred_ci.index,\n",
    "                        pred_ci.iloc[:, 0],\n",
    "                        pred_ci.iloc[:, 1], \n",
    "                        color='k', alpha=.2)\n",
    "\n",
    "        ax.set_ylabel(f'Housing Value for {zipcode}')\n",
    "        ax.set_title(f'Model Validation for Zipcode {zipcode}')\n",
    "\n",
    "\n",
    "    # Now use the matplotlib .remove() method to \n",
    "    # delete anything we didn't use\n",
    "    for ax in axes_list:\n",
    "        ax.remove()\n",
    "\n",
    "def plot_roi(best_results_cvrmse, ts_all):\n",
    "    fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(15,36))\n",
    "    axes_list = [item for sublist in axes for item in sublist] \n",
    "\n",
    "    for zipcode in best_results_cvrmse.zipcode:\n",
    "        ax = axes_list.pop(0)\n",
    "        ROI_1yr = 100 * (ts_all[zipcode].tshift(-12) / ts_all[zipcode] - 1)\n",
    "        ROI_1yr.plot(x='Year', y='ROI', label='1 Year ROI', ax=ax, legend=True)\n",
    "        ROI_2yr = 100 * (ts_all[zipcode].tshift(-24) / ts_all[zipcode] - 1)\n",
    "        ROI_2yr.plot(x='Year', y='ROI', label='2 Year ROI', ax=ax, legend=True)\n",
    "        ax.set_title(zipcode)\n",
    "        ax.axhline(c='black')\n",
    "        ax.tick_params(\n",
    "            which='both',\n",
    "            bottom='on',\n",
    "            left='on',\n",
    "            right='off',\n",
    "            top='off'\n",
    "        )\n",
    "        ax.set_ylim(-30, 70)\n",
    "        ax.set_ylabel('Return on Investment')\n",
    "        #ax.spines['left'].set_position('zero')\n",
    "\n",
    "        #ax.spines['top'].set_visible(False)\n",
    "        #ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "    # Now use the matplotlib .remove() method to \n",
    "    # delete anything we didn't use\n",
    "    for ax in axes_list:\n",
    "        ax.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook time_series_analysis.ipynb to script\n",
      "[NbConvertApp] Writing 15023 bytes to time_series_analysis.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script time_series_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
